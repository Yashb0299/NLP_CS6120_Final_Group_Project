{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox_dkPVQV8g_"
      },
      "source": [
        "### **NLP Final Project**\n",
        "#### **Spam and Sentiment Email Analysis: Spam Bi-LSTM Supervised Learning**\n",
        "\n",
        "Wilson Neira"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-53iXbvV8hC"
      },
      "source": [
        "##### **1. Import**\n",
        "* Import libraries needed for deep learning and text sequence preparation with TensorFlow/Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BdgW2N01V8hD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Concatenate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz-8VL-5V8hG"
      },
      "source": [
        "##### **2. Tokenization and Sequence Padding**\n",
        "\n",
        "* Load datasets, encode labels, convert email texts into padded numeric sequences using Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8hoRudKgV8hG"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "train_df = pd.read_csv(\"group34_train_data_with_clusters.csv\")\n",
        "test_df = pd.read_csv(\"group34_test_data_with_clusters.csv\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "train_labels = le.fit_transform(train_df['label'])  # spam:1, ham:0\n",
        "test_labels = le.transform(test_df['label'])\n",
        "\n",
        "# Prepare tokenizer (fit on train)\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_df['email'])\n",
        "\n",
        "# Text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_df['email'])\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_df['email'])\n",
        "\n",
        "# Padding sequences\n",
        "max_len = 200\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZ0gKi7V8hI"
      },
      "source": [
        "##### **3. Bi-LSTM Model Definition**\n",
        "* Defined a flexible Bi-LSTM model that can incorporate clustering features through concatenation for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hXzN3soxV8hJ"
      },
      "outputs": [],
      "source": [
        "# Function to create Bi-LSTM model\n",
        "def create_model(cluster_feature_dim=0):\n",
        "    input_text = Input(shape=(max_len,))\n",
        "    embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
        "    x = Bidirectional(LSTM(64))(embedding)\n",
        "\n",
        "    if cluster_feature_dim > 0:\n",
        "        input_cluster = Input(shape=(cluster_feature_dim,))\n",
        "        concatenated = Concatenate()([x, input_cluster])\n",
        "        output = Dense(1, activation='sigmoid')(concatenated)\n",
        "        model = Model(inputs=[input_text, input_cluster], outputs=output)\n",
        "    else:\n",
        "        output = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_text, outputs=output)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ6ABwUBoWmd"
      },
      "source": [
        "##### **4. Prepare Clustering Features and Configurations**\n",
        "* Perform one-hot encoding on K-Means and hierarchical cluster labels, prepare datasets for each model configuration and set up the StratifiedKFold for cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bPqF7_RVV8hJ"
      },
      "outputs": [],
      "source": [
        "# Prepare clustering features\n",
        "cluster_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "train_kmeans_feat = cluster_encoder.fit_transform(train_df[['kmeans_cluster']])\n",
        "test_kmeans_feat = cluster_encoder.transform(test_df[['kmeans_cluster']])\n",
        "\n",
        "train_hier_feat = cluster_encoder.fit_transform(train_df[['hierarchical_cluster']])\n",
        "test_hier_feat = cluster_encoder.transform(test_df[['hierarchical_cluster']])\n",
        "\n",
        "# Combine K-Means and Hierarchical clustering features\n",
        "train_combined_feat = np.hstack((train_kmeans_feat, train_hier_feat))\n",
        "test_combined_feat = np.hstack((test_kmeans_feat, test_hier_feat))\n",
        "\n",
        "# K-fold Cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Store configurations with their features\n",
        "configurations = {\n",
        "    'Baseline Bi-LSTM': (X_train_pad, None, None),\n",
        "    'Bi-LSTM + K-Means': (X_train_pad, train_kmeans_feat, test_kmeans_feat),\n",
        "    'Bi-LSTM + Hierarchical': (X_train_pad, train_hier_feat, test_hier_feat),\n",
        "    'Bi-LSTM + Combined Clusters': (X_train_pad, train_combined_feat, test_combined_feat)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVM37ynLoWme"
      },
      "source": [
        "##### **5. Stratified 5-Fold Cross-Validation**\n",
        "* Implement a 5-fold cross-validation strategy for each model configuration, train and validate the models and report classification metrics for each configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1vIjUDKV8hK",
        "outputId": "4b761bbd-7304-4c27-9dd9-55876e07069f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Stratified 5-Fold CV: Baseline Bi-LSTM \n",
            "\n",
            "Fold 1\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - accuracy: 0.8862 - loss: 0.2657 - val_accuracy: 0.9824 - val_loss: 0.0530\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9922 - loss: 0.0258 - val_accuracy: 0.9839 - val_loss: 0.0510\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9959 - loss: 0.0149 - val_accuracy: 0.9820 - val_loss: 0.0561\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8609 - loss: 0.3122 - val_accuracy: 0.9752 - val_loss: 0.0802\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9906 - loss: 0.0346 - val_accuracy: 0.9841 - val_loss: 0.0616\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9961 - loss: 0.0149 - val_accuracy: 0.9824 - val_loss: 0.0638\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8960 - loss: 0.2511 - val_accuracy: 0.9841 - val_loss: 0.0476\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9892 - loss: 0.0355 - val_accuracy: 0.9841 - val_loss: 0.0478\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9947 - loss: 0.0187 - val_accuracy: 0.9822 - val_loss: 0.0502\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8947 - loss: 0.2613 - val_accuracy: 0.9850 - val_loss: 0.0521\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9914 - loss: 0.0327 - val_accuracy: 0.9848 - val_loss: 0.0512\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9943 - loss: 0.0202 - val_accuracy: 0.9841 - val_loss: 0.0494\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8726 - loss: 0.2716 - val_accuracy: 0.9859 - val_loss: 0.0434\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9916 - loss: 0.0270 - val_accuracy: 0.9848 - val_loss: 0.0447\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9945 - loss: 0.0185 - val_accuracy: 0.9865 - val_loss: 0.0475\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Classification Report for Baseline Bi-LSTM (5-fold CV combined results):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.98      0.98     13236\n",
            "        spam       0.98      0.99      0.98     13736\n",
            "\n",
            "    accuracy                           0.98     26972\n",
            "   macro avg       0.98      0.98      0.98     26972\n",
            "weighted avg       0.98      0.98      0.98     26972\n",
            "\n",
            "\n",
            " Stratified 5-Fold CV: Bi-LSTM + K-Means \n",
            "\n",
            "Fold 1\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8500 - loss: 0.2698 - val_accuracy: 0.9577 - val_loss: 0.1262\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9716 - loss: 0.0819 - val_accuracy: 0.9800 - val_loss: 0.0603\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9924 - loss: 0.0249 - val_accuracy: 0.9837 - val_loss: 0.0654\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8990 - loss: 0.2545 - val_accuracy: 0.9815 - val_loss: 0.0568\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.9930 - loss: 0.0225 - val_accuracy: 0.9837 - val_loss: 0.0595\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9959 - loss: 0.0154 - val_accuracy: 0.9842 - val_loss: 0.0652\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8703 - loss: 0.2861 - val_accuracy: 0.9818 - val_loss: 0.0564\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9899 - loss: 0.0321 - val_accuracy: 0.9844 - val_loss: 0.0469\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9957 - loss: 0.0174 - val_accuracy: 0.9848 - val_loss: 0.0490\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.8393 - loss: 0.2751 - val_accuracy: 0.9833 - val_loss: 0.0481\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9923 - loss: 0.0268 - val_accuracy: 0.9848 - val_loss: 0.0477\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9943 - loss: 0.0186 - val_accuracy: 0.9859 - val_loss: 0.0509\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9466 - loss: 0.2209 - val_accuracy: 0.9863 - val_loss: 0.0398\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9919 - loss: 0.0299 - val_accuracy: 0.9889 - val_loss: 0.0338\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9959 - loss: 0.0149 - val_accuracy: 0.9822 - val_loss: 0.0483\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Classification Report for Bi-LSTM + K-Means (5-fold CV combined results):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.98      0.98     13236\n",
            "        spam       0.98      0.99      0.98     13736\n",
            "\n",
            "    accuracy                           0.98     26972\n",
            "   macro avg       0.98      0.98      0.98     26972\n",
            "weighted avg       0.98      0.98      0.98     26972\n",
            "\n",
            "\n",
            " Stratified 5-Fold CV: Bi-LSTM + Hierarchical \n",
            "\n",
            "Fold 1\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.8880 - loss: 0.2619 - val_accuracy: 0.9842 - val_loss: 0.0541\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9931 - loss: 0.0257 - val_accuracy: 0.9839 - val_loss: 0.0533\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9956 - loss: 0.0162 - val_accuracy: 0.9837 - val_loss: 0.0673\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.7814 - loss: 0.2962 - val_accuracy: 0.9829 - val_loss: 0.0585\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9922 - loss: 0.0291 - val_accuracy: 0.9835 - val_loss: 0.0579\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9953 - loss: 0.0193 - val_accuracy: 0.9774 - val_loss: 0.0953\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8309 - loss: 0.2946 - val_accuracy: 0.9765 - val_loss: 0.0714\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9887 - loss: 0.0375 - val_accuracy: 0.9857 - val_loss: 0.0436\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9950 - loss: 0.0184 - val_accuracy: 0.9846 - val_loss: 0.0452\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9389 - loss: 0.2430 - val_accuracy: 0.9826 - val_loss: 0.0543\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9915 - loss: 0.0302 - val_accuracy: 0.9846 - val_loss: 0.0490\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9960 - loss: 0.0146 - val_accuracy: 0.9850 - val_loss: 0.0481\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9116 - loss: 0.2461 - val_accuracy: 0.9837 - val_loss: 0.0463\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9917 - loss: 0.0269 - val_accuracy: 0.9859 - val_loss: 0.0420\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9955 - loss: 0.0147 - val_accuracy: 0.9874 - val_loss: 0.0404\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Classification Report for Bi-LSTM + Hierarchical (5-fold CV combined results):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.98      0.98     13236\n",
            "        spam       0.98      0.99      0.98     13736\n",
            "\n",
            "    accuracy                           0.98     26972\n",
            "   macro avg       0.98      0.98      0.98     26972\n",
            "weighted avg       0.98      0.98      0.98     26972\n",
            "\n",
            "\n",
            " Stratified 5-Fold CV: Bi-LSTM + Combined Clusters \n",
            "\n",
            "Fold 1\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8961 - loss: 0.2466 - val_accuracy: 0.9807 - val_loss: 0.0616\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9924 - loss: 0.0279 - val_accuracy: 0.9861 - val_loss: 0.0492\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9960 - loss: 0.0140 - val_accuracy: 0.9835 - val_loss: 0.0499\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "\n",
            "Fold 2\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8205 - loss: 0.3114 - val_accuracy: 0.9846 - val_loss: 0.0529\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9913 - loss: 0.0299 - val_accuracy: 0.9829 - val_loss: 0.0561\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9963 - loss: 0.0151 - val_accuracy: 0.9813 - val_loss: 0.0747\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 3\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9399 - loss: 0.2264 - val_accuracy: 0.9811 - val_loss: 0.0503\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9914 - loss: 0.0289 - val_accuracy: 0.9831 - val_loss: 0.0457\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9951 - loss: 0.0174 - val_accuracy: 0.9826 - val_loss: 0.0505\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "\n",
            "Fold 4\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8968 - loss: 0.2533 - val_accuracy: 0.9857 - val_loss: 0.0473\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9916 - loss: 0.0270 - val_accuracy: 0.9846 - val_loss: 0.0451\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9956 - loss: 0.0162 - val_accuracy: 0.9850 - val_loss: 0.0514\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "\n",
            "Fold 5\n",
            "Epoch 1/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9507 - loss: 0.2511 - val_accuracy: 0.9867 - val_loss: 0.0410\n",
            "Epoch 2/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9906 - loss: 0.0308 - val_accuracy: 0.9881 - val_loss: 0.0362\n",
            "Epoch 3/3\n",
            "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9957 - loss: 0.0145 - val_accuracy: 0.9881 - val_loss: 0.0452\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Classification Report for Bi-LSTM + Combined Clusters (5-fold CV combined results):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.98      0.98     13236\n",
            "        spam       0.98      0.99      0.98     13736\n",
            "\n",
            "    accuracy                           0.98     26972\n",
            "   macro avg       0.98      0.98      0.98     26972\n",
            "weighted avg       0.98      0.98      0.98     26972\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Perform StratifiedKFold cross-validation for each configuration\n",
        "for config_name, (text_data, cluster_data, test_cluster_data) in configurations.items():\n",
        "    print(f\"\\n Stratified 5-Fold CV: {config_name} \")\n",
        "\n",
        "    all_val_preds = []\n",
        "    all_val_actuals = []\n",
        "    fold = 1\n",
        "    for train_index, val_index in skf.split(text_data, train_labels):\n",
        "        print(f\"\\nFold {fold}\")\n",
        "        X_text_train, X_text_val = text_data[train_index], text_data[val_index]\n",
        "        y_train_fold, y_val_fold = train_labels[train_index], train_labels[val_index]\n",
        "\n",
        "        # Prepare clustering data if possible\n",
        "        if cluster_data is not None:\n",
        "            X_cluster_train, X_cluster_val = cluster_data[train_index], cluster_data[val_index]\n",
        "            model = create_model(cluster_feature_dim=cluster_data.shape[1])\n",
        "            model.fit(\n",
        "                [X_text_train, X_cluster_train], y_train_fold,\n",
        "                epochs=3, batch_size=64,\n",
        "                validation_data=([X_text_val, X_cluster_val], y_val_fold)\n",
        "            )\n",
        "            val_pred = (model.predict([X_text_val, X_cluster_val]) > 0.5).astype(\"int32\")\n",
        "        else:\n",
        "            model = create_model()\n",
        "            model.fit(\n",
        "                X_text_train, y_train_fold,\n",
        "                epochs=3, batch_size=64,\n",
        "                validation_data=(X_text_val, y_val_fold)\n",
        "            )\n",
        "            val_pred = (model.predict(X_text_val) > 0.5).astype(\"int32\")\n",
        "\n",
        "        all_val_preds.extend(val_pred.flatten())\n",
        "        all_val_actuals.extend(y_val_fold)\n",
        "        fold += 1\n",
        "\n",
        "    # Classification report after cross-validation for each configuration\n",
        "    print(f\"\\nClassification Report for {config_name} (5-fold CV combined results):\")\n",
        "    print(classification_report(all_val_actuals, all_val_preds, target_names=le.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvmAM5lSoWme"
      },
      "source": [
        "##### **6. Final Explicit Evaluation on Unseen Test Set**\n",
        "* Retrain each model configuration explicitly on the full training set, evaluate them on an independent test set, and provide detailed classification metrics along with insightful extreme-error analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqP3fNzOV8hL",
        "outputId": "80a2fa1a-9013-40ee-9b6c-b9b9cc6d832e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " FINAL EVALUATION ON UNSEEN TEST SET \n",
            "\n",
            "\n",
            " Final Classification Report: Baseline Bi-LSTM \n",
            "Epoch 1/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8823 - loss: 0.2706 - val_accuracy: 0.9874 - val_loss: 0.0427\n",
            "Epoch 2/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9907 - loss: 0.0311 - val_accuracy: 0.9867 - val_loss: 0.0425\n",
            "Epoch 3/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9948 - loss: 0.0192 - val_accuracy: 0.9885 - val_loss: 0.0364\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99      3309\n",
            "        spam       0.99      0.99      0.99      3434\n",
            "\n",
            "    accuracy                           0.99      6743\n",
            "   macro avg       0.99      0.99      0.99      6743\n",
            "weighted avg       0.99      0.99      0.99      6743\n",
            "\n",
            "\n",
            "Insightful Analysis of Extreme Errors for Baseline Bi-LSTM (Top 10 examples):\n",
            "                                                 email  actual  prediction\n",
            "38   subject reply inquiry hjl turn days paypal sig...       1           0\n",
            "147  subject fw whose needs perfect proprietary hus...       0           1\n",
            "164  subject personal trainers like time read reall...       0           1\n",
            "259  subject fw quips remember amateurs built ark p...       0           1\n",
            "269  subject ds need one place congresswoman one re...       1           0\n",
            "300  subject pu na turalbutpow erful thought taking...       1           0\n",
            "426  subject enron mentions usa u stocks slip fed r...       0           1\n",
            "616  subject introducing new iijournals online inst...       0           1\n",
            "691  subject fw computers happy holidays bonnie hit...       0           1\n",
            "741  subject positions available microsoft research...       1           0\n",
            "\n",
            " Final Classification Report: Bi-LSTM + K-Means \n",
            "Epoch 1/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8410 - loss: 0.2366 - val_accuracy: 0.9863 - val_loss: 0.0432\n",
            "Epoch 2/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9916 - loss: 0.0288 - val_accuracy: 0.9804 - val_loss: 0.0599\n",
            "Epoch 3/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9937 - loss: 0.0206 - val_accuracy: 0.9889 - val_loss: 0.0376\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99      3309\n",
            "        spam       0.99      0.99      0.99      3434\n",
            "\n",
            "    accuracy                           0.99      6743\n",
            "   macro avg       0.99      0.99      0.99      6743\n",
            "weighted avg       0.99      0.99      0.99      6743\n",
            "\n",
            "\n",
            "Insightful Analysis of Extreme Errors for Bi-LSTM + K-Means (Top 10 examples):\n",
            "                                                  email  actual  prediction\n",
            "147   subject fw whose needs perfect proprietary hus...       0           1\n",
            "259   subject fw quips remember amateurs built ark p...       0           1\n",
            "269   subject ds need one place congresswoman one re...       1           0\n",
            "299   subject aluum llgra caalls levltrra xana loraa...       1           0\n",
            "426   subject enron mentions usa u stocks slip fed r...       0           1\n",
            "616   subject introducing new iijournals online inst...       0           1\n",
            "691   subject fw computers happy holidays bonnie hit...       0           1\n",
            "741   subject positions available microsoft research...       1           0\n",
            "1074  subject attention pr department attention pr d...       1           0\n",
            "1084                                 subject geir goals       0           1\n",
            "\n",
            " Final Classification Report: Bi-LSTM + Hierarchical \n",
            "Epoch 1/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8849 - loss: 0.2681 - val_accuracy: 0.9807 - val_loss: 0.0554\n",
            "Epoch 2/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9903 - loss: 0.0325 - val_accuracy: 0.9900 - val_loss: 0.0346\n",
            "Epoch 3/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9956 - loss: 0.0160 - val_accuracy: 0.9874 - val_loss: 0.0410\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.98      0.99      3309\n",
            "        spam       0.98      0.99      0.99      3434\n",
            "\n",
            "    accuracy                           0.99      6743\n",
            "   macro avg       0.99      0.99      0.99      6743\n",
            "weighted avg       0.99      0.99      0.99      6743\n",
            "\n",
            "\n",
            "Insightful Analysis of Extreme Errors for Bi-LSTM + Hierarchical (Top 10 examples):\n",
            "                                                 email  actual  prediction\n",
            "38   subject reply inquiry hjl turn days paypal sig...       1           0\n",
            "147  subject fw whose needs perfect proprietary hus...       0           1\n",
            "188  subject attack powerpoint installed go http of...       0           1\n",
            "259  subject fw quips remember amateurs built ark p...       0           1\n",
            "426  subject enron mentions usa u stocks slip fed r...       0           1\n",
            "476  subject external counterparty external user lo...       0           1\n",
            "616  subject introducing new iijournals online inst...       0           1\n",
            "691  subject fw computers happy holidays bonnie hit...       0           1\n",
            "714  subject correction first delivery cody read cu...       0           1\n",
            "741  subject positions available microsoft research...       1           0\n",
            "\n",
            " Final Classification Report: Bi-LSTM + Combined Clusters \n",
            "Epoch 1/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8371 - loss: 0.3082 - val_accuracy: 0.9818 - val_loss: 0.0473\n",
            "Epoch 2/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9894 - loss: 0.0352 - val_accuracy: 0.9885 - val_loss: 0.0375\n",
            "Epoch 3/3\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9962 - loss: 0.0153 - val_accuracy: 0.9900 - val_loss: 0.0326\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.98      0.99      3309\n",
            "        spam       0.98      0.99      0.99      3434\n",
            "\n",
            "    accuracy                           0.99      6743\n",
            "   macro avg       0.99      0.99      0.99      6743\n",
            "weighted avg       0.99      0.99      0.99      6743\n",
            "\n",
            "\n",
            "Insightful Analysis of Extreme Errors for Bi-LSTM + Combined Clusters (Top 10 examples):\n",
            "                                                 email  actual  prediction\n",
            "147  subject fw whose needs perfect proprietary hus...       0           1\n",
            "164  subject personal trainers like time read reall...       0           1\n",
            "259  subject fw quips remember amateurs built ark p...       0           1\n",
            "269  subject ds need one place congresswoman one re...       1           0\n",
            "313  subject hi hello mister bill quick note say so...       0           1\n",
            "426  subject enron mentions usa u stocks slip fed r...       0           1\n",
            "476  subject external counterparty external user lo...       0           1\n",
            "478  subject bridge errors keep eye bridge still er...       0           1\n",
            "616  subject introducing new iijournals online inst...       0           1\n",
            "691  subject fw computers happy holidays bonnie hit...       0           1\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# Final evaluation for each configuration clearly:\n",
        "print(\"\\n FINAL EVALUATION ON UNSEEN TEST SET \\n\")\n",
        "\n",
        "for config_name, (text_data, cluster_data, test_cluster_data) in configurations.items():\n",
        "    print(f\"\\n Final Classification Report: {config_name} \")\n",
        "\n",
        "    # Train on full training data\n",
        "    if cluster_data is not None:\n",
        "        model = create_model(cluster_feature_dim=cluster_data.shape[1])\n",
        "        model.fit(\n",
        "            [text_data, cluster_data], train_labels,\n",
        "            epochs=3, batch_size=64, validation_split=0.1\n",
        "        )\n",
        "        predictions = (model.predict([X_test_pad, test_cluster_data]) > 0.5).astype(\"int32\")\n",
        "    else:\n",
        "        model = create_model()\n",
        "        model.fit(\n",
        "            text_data, train_labels,\n",
        "            epochs=3, batch_size=64, validation_split=0.1\n",
        "        )\n",
        "        predictions = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Print final classification report\n",
        "    print(classification_report(test_labels, predictions, target_names=le.classes_))\n",
        "\n",
        "    # Extreme error analysis\n",
        "    error_analysis_df = test_df.copy()\n",
        "    error_analysis_df['prediction'] = predictions.flatten()\n",
        "    error_analysis_df['actual'] = test_labels\n",
        "    extreme_errors = error_analysis_df[error_analysis_df['prediction'] != error_analysis_df['actual']]\n",
        "\n",
        "    print(f\"\\nInsightful Analysis of Extreme Errors for {config_name} (Top 10 examples):\")\n",
        "    print(extreme_errors[['email', 'actual', 'prediction']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOyhdNugV8hQ"
      },
      "source": [
        "##### **7. Results**\n",
        "4 configurations were evaluated using a Bi-LSTM model with 5-fold cross-validation and an independent test set.\n",
        "\n",
        "### Cross-Validation Results\n",
        "\n",
        "* **Baseline Bi-LSTM**:\n",
        "  * Accuracy ranged between 98.0-98.6%, with loss consistently low (0.043-0.056).\n",
        "  * High precision, recall, and F1-score for both classes:\n",
        "    * ham: 99% precision, 98% recall, 98% F1\n",
        "    * spam: 98% precision, 99% recall, 98% F1\n",
        "\n",
        "* **Bi-LSTM + K-Means**:\n",
        "  * Similar accuracy (mostly 98.0-98.8%) and low loss (0.039-0.065).\n",
        "  * Slightly better variance handling across folds.\n",
        "  * ham: 99% precision, 98% recall, 98% F1\n",
        "  * spam: 98% precision, 99% recall, 98% F1\n",
        "\n",
        "* **Bi-LSTM + Hierarchical**:\n",
        "  * Performance close to baseline and K-Means. Accuracy hovered around 98-98.7%, loss within 0.041-0.067.\n",
        "  * ham: 99% precision, 98% recall, 99% F1\n",
        "  * spam: 98% precision, 99% recall, 99% F1\n",
        "\n",
        "* **Bi-LSTM + Combined Clusters**:\n",
        "  * Accuracy and loss comparable to the best-performing models (accuracy 98-99%, loss 0.032-0.050).\n",
        "  * F1 scores identical to other configs:\n",
        "    * ham: 99% precision, 98% recall, 99% F1\n",
        "    * spam: 98% precision, 99% recall, 99% F1\n",
        "\n",
        "### Final Evaluation on Unseen Test Set\n",
        "\n",
        "* **Baseline Bi-LSTM**:\n",
        "  * Accuracy: 99%, Loss: 0.036\n",
        "  * ham/spam: 99% precision, 99% recall, 99% F1\n",
        "\n",
        "* **Bi-LSTM + K-Means**:\n",
        "  * Accuracy: 99%, Loss: 0.038\n",
        "  * ham/spam: 99% precision, 99% recall, 99% F1\n",
        "\n",
        "* **Bi-LSTM + Hierarchical**:\n",
        "  * Accuracy: 99%, Loss: 0.041\n",
        "  * ham: 99% precision, 98% recall, 99% F1\n",
        "  * spam: 98% precision, 99% recall, 99% F1\n",
        "\n",
        "* **Bi-LSTM + Combined Clusters**:\n",
        "  * Accuracy: 99%, Lowest Loss: 0.033\n",
        "  * ham: 99% precision, 98% recall, 99% F1\n",
        "  * spam: 98% precision, 99% recall, 99% F1\n",
        "\n",
        "### Bias-Variance Analysis\n",
        "\n",
        "* **Baseline Bi-LSTM** shows **low bias** (good training performance) and **low variance** (generalizes well to test set). Minimal overfitting observed.\n",
        "\n",
        "* **Bi-LSTM + K-Means** adds slight regularization with clustering, which maintains generalization. Its initial training accuracy starts lower, but rapidly improves — indicating **slightly reduced variance**.\n",
        "\n",
        "* **Bi-LSTM + Hierarchical** introduces more variance at the beginning (some inconsistent early training losses), but achieves excellent test accuracy indicating **slightly higher variance**, yet still balanced.\n",
        "\n",
        "* **Bi-LSTM + Combined Clusters** performs equally or slightly better than others in loss, with balanced training/test performance. Suggests **low bias** and possibly even improved robustness, due to richer feature representations.\n",
        "\n",
        "### Extreme Error Analysis\n",
        "Across all models, the misclassified examples remain mostly consistent and tend to involve ambiguous or complex language. This could imply that the model’s performance limit is more due to data complexity than model bias or variance.\n",
        "\n",
        "### Summary\n",
        "All configurations perform very well. The combined cluster configuration shows small improvements in generalization and loss, suggesting that combining clustering techniques can slightly enhance robustness. However, given the near-perfect scores across all models, the baseline Bi-LSTM already captures core patterns effectively. Clustering features may not offer much benefit on this dataset, but they help slightly with variance reduction and model stability.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
