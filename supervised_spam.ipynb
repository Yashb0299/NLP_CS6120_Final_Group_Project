{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NLP Final Project**\n",
    "\n",
    "Wilson Neira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Concatenate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(\"train_data_with_clusters.csv\")\n",
    "test_df = pd.read_csv(\"test_data_with_clusters.csv\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(train_df['label'])  # spam:1, ham:0\n",
    "test_labels = le.transform(test_df['label'])\n",
    "\n",
    "# Prepare tokenizer (fit on train)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['email'])\n",
    "\n",
    "# Text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['email'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['email'])\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bi-LSTM Baseline (no clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 145ms/step - accuracy: 0.8677 - loss: 0.2935 - val_accuracy: 0.9867 - val_loss: 0.0430\n",
      "Epoch 2/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 139ms/step - accuracy: 0.9905 - loss: 0.0309 - val_accuracy: 0.9881 - val_loss: 0.0353\n",
      "Epoch 3/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 147ms/step - accuracy: 0.9944 - loss: 0.0202 - val_accuracy: 0.9818 - val_loss: 0.0536\n",
      "Epoch 4/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 153ms/step - accuracy: 0.9956 - loss: 0.0149 - val_accuracy: 0.9870 - val_loss: 0.0578\n",
      "Epoch 5/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 156ms/step - accuracy: 0.9976 - loss: 0.0086 - val_accuracy: 0.9896 - val_loss: 0.0407\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step\n",
      "Baseline Bi-LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99      3309\n",
      "        spam       0.98      0.99      0.99      3434\n",
      "\n",
      "    accuracy                           0.99      6743\n",
      "   macro avg       0.99      0.99      0.99      6743\n",
      "weighted avg       0.99      0.99      0.99      6743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input_text = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
    "x = Bidirectional(LSTM(64))(embedding)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_baseline = Model(inputs=input_text, outputs=output)\n",
    "model_baseline.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model_baseline.fit(X_train_pad, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "predictions = (model_baseline.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "print(\"Baseline Bi-LSTM Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Bi-LSTM with K-Means Cluster Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 146ms/step - accuracy: 0.8921 - loss: 0.2321 - val_accuracy: 0.9889 - val_loss: 0.0376\n",
      "Epoch 2/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 143ms/step - accuracy: 0.9912 - loss: 0.0281 - val_accuracy: 0.9863 - val_loss: 0.0366\n",
      "Epoch 3/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 144ms/step - accuracy: 0.9947 - loss: 0.0170 - val_accuracy: 0.9878 - val_loss: 0.0434\n",
      "Epoch 4/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 144ms/step - accuracy: 0.9963 - loss: 0.0109 - val_accuracy: 0.9874 - val_loss: 0.0370\n",
      "Epoch 5/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 146ms/step - accuracy: 0.9973 - loss: 0.0093 - val_accuracy: 0.9855 - val_loss: 0.0453\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step\n",
      "Bi-LSTM + Clusters Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99      3309\n",
      "        spam       0.99      0.99      0.99      3434\n",
      "\n",
      "    accuracy                           0.99      6743\n",
      "   macro avg       0.99      0.99      0.99      6743\n",
      "weighted avg       0.99      0.99      0.99      6743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare cluster features (one-hot encoding)\n",
    "cluster_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# KMeans clusters as example \n",
    "train_cluster_feat = cluster_encoder.fit_transform(train_df[['kmeans_cluster']])\n",
    "test_cluster_feat = cluster_encoder.transform(test_df[['kmeans_cluster']])\n",
    "\n",
    "# Model definition (text + cluster)\n",
    "input_text = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
    "x = Bidirectional(LSTM(64))(embedding)\n",
    "\n",
    "# Cluster input\n",
    "input_cluster = Input(shape=(train_cluster_feat.shape[1],))\n",
    "\n",
    "# Concatenate clusters with Bi-LSTM output\n",
    "concatenated = Concatenate()([x, input_cluster])\n",
    "\n",
    "# Dense layers\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "model_clusters = Model(inputs=[input_text, input_cluster], outputs=output)\n",
    "model_clusters.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model_clusters.fit(\n",
    "    [X_train_pad, train_cluster_feat], \n",
    "    train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "predictions = (model_clusters.predict([X_test_pad, test_cluster_feat]) > 0.5).astype(\"int32\")\n",
    "print(\"Bi-LSTM + Clusters Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bi-LSTM with Hierarchical Cluster Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 139ms/step - accuracy: 0.8949 - loss: 0.2359 - val_accuracy: 0.9874 - val_loss: 0.0378\n",
      "Epoch 2/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 139ms/step - accuracy: 0.9919 - loss: 0.0268 - val_accuracy: 0.9896 - val_loss: 0.0301\n",
      "Epoch 3/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 140ms/step - accuracy: 0.9957 - loss: 0.0145 - val_accuracy: 0.9904 - val_loss: 0.0307\n",
      "Epoch 4/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 140ms/step - accuracy: 0.9971 - loss: 0.0101 - val_accuracy: 0.9900 - val_loss: 0.0312\n",
      "Epoch 5/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 146ms/step - accuracy: 0.9984 - loss: 0.0060 - val_accuracy: 0.9889 - val_loss: 0.0352\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step\n",
      "Bi-LSTM + Hierarchical Clusters Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.99      0.98      3309\n",
      "        spam       0.99      0.98      0.99      3434\n",
      "\n",
      "    accuracy                           0.99      6743\n",
      "   macro avg       0.99      0.99      0.99      6743\n",
      "weighted avg       0.99      0.99      0.99      6743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare cluster features (one-hot encoding)\n",
    "cluster_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# KMeans clusters as example \n",
    "train_cluster_feat = cluster_encoder.fit_transform(train_df[['hierarchical_cluster']])\n",
    "test_cluster_feat = cluster_encoder.transform(test_df[['hierarchical_cluster']])\n",
    "\n",
    "# Model definition (text + cluster)\n",
    "input_text = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
    "x = Bidirectional(LSTM(64))(embedding)\n",
    "\n",
    "# Cluster input\n",
    "input_cluster = Input(shape=(train_cluster_feat.shape[1],))\n",
    "\n",
    "# Concatenate clusters with Bi-LSTM output\n",
    "concatenated = Concatenate()([x, input_cluster])\n",
    "\n",
    "# Dense layers\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "model_clusters = Model(inputs=[input_text, input_cluster], outputs=output)\n",
    "model_clusters.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model_clusters.fit(\n",
    "    [X_train_pad, train_cluster_feat], \n",
    "    train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "predictions = (model_clusters.predict([X_test_pad, test_cluster_feat]) > 0.5).astype(\"int32\")\n",
    "print(\"Bi-LSTM + Hierarchical Clusters Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
