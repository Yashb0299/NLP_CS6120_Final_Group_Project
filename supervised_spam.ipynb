{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLP Final Project**\n",
    "#### **Spam and Sentiment Email Analysis: Spam Bi-LSTM Supervised Learning**\n",
    "\n",
    "Wilson Neira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Import**\n",
    "* Import libraries needed for deep learning and text sequence preparation with TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Concatenate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Tokenization and Sequence Padding**\n",
    "\n",
    "* Load datasets, encode labels, convert email texts into padded numeric sequences using Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(\"train_data_with_clusters.csv\")\n",
    "test_df = pd.read_csv(\"test_data_with_clusters.csv\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(train_df['label'])  # spam:1, ham:0\n",
    "test_labels = le.transform(test_df['label'])\n",
    "\n",
    "# Prepare tokenizer (fit on train)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['email'])\n",
    "\n",
    "# Text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['email'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['email'])\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Bi-LSTM Baseline (no clusters)**\n",
    "* Define, train, and evaluate a baseline Bi-LSTM model on email sequences, reporting classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 156ms/step - accuracy: 0.8809 - loss: 0.2738 - val_accuracy: 0.9837 - val_loss: 0.0514\n",
      "Epoch 2/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 154ms/step - accuracy: 0.9790 - loss: 0.0611 - val_accuracy: 0.9867 - val_loss: 0.0433\n",
      "Epoch 3/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 160ms/step - accuracy: 0.9827 - loss: 0.0524 - val_accuracy: 0.9896 - val_loss: 0.0379\n",
      "Epoch 4/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 165ms/step - accuracy: 0.9961 - loss: 0.0143 - val_accuracy: 0.9885 - val_loss: 0.0439\n",
      "Epoch 5/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 161ms/step - accuracy: 0.9976 - loss: 0.0117 - val_accuracy: 0.9841 - val_loss: 0.0559\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step\n",
      "Baseline Bi-LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.97      0.98      3309\n",
      "        spam       0.97      0.99      0.98      3434\n",
      "\n",
      "    accuracy                           0.98      6743\n",
      "   macro avg       0.98      0.98      0.98      6743\n",
      "weighted avg       0.98      0.98      0.98      6743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "input_text = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
    "x = Bidirectional(LSTM(64))(embedding)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_baseline = Model(inputs=input_text, outputs=output)\n",
    "model_baseline.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model_baseline.fit(X_train_pad, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "predictions = (model_baseline.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "print(\"Baseline Bi-LSTM Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4. Bi-LSTM with K-Means Cluster Features**\n",
    "Add K-Means cluster labels (one-hot encoded) as extra features, defining a combined model (text + clusters), train and evaluate it, to print performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 174ms/step - accuracy: 0.8792 - loss: 0.2405 - val_accuracy: 0.9844 - val_loss: 0.0512\n",
      "Epoch 2/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 174ms/step - accuracy: 0.9899 - loss: 0.0343 - val_accuracy: 0.9904 - val_loss: 0.0338\n",
      "Epoch 3/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 174ms/step - accuracy: 0.9957 - loss: 0.0169 - val_accuracy: 0.9907 - val_loss: 0.0430\n",
      "Epoch 4/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 176ms/step - accuracy: 0.9969 - loss: 0.0118 - val_accuracy: 0.9889 - val_loss: 0.0431\n",
      "Epoch 5/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 184ms/step - accuracy: 0.9985 - loss: 0.0068 - val_accuracy: 0.9881 - val_loss: 0.0390\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step\n",
      "Bi-LSTM + Clusters Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99      3309\n",
      "        spam       0.98      0.99      0.99      3434\n",
      "\n",
      "    accuracy                           0.99      6743\n",
      "   macro avg       0.99      0.99      0.99      6743\n",
      "weighted avg       0.99      0.99      0.99      6743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare cluster features (one-hot encoding)\n",
    "cluster_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# KMeans clusters as example \n",
    "train_cluster_feat = cluster_encoder.fit_transform(train_df[['kmeans_cluster']])\n",
    "test_cluster_feat = cluster_encoder.transform(test_df[['kmeans_cluster']])\n",
    "\n",
    "# Model definition (text + cluster)\n",
    "input_text = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
    "x = Bidirectional(LSTM(64))(embedding)\n",
    "\n",
    "# Cluster input\n",
    "input_cluster = Input(shape=(train_cluster_feat.shape[1],))\n",
    "\n",
    "# Concatenate clusters with Bi-LSTM output\n",
    "concatenated = Concatenate()([x, input_cluster])\n",
    "\n",
    "# Dense layers\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "model_clusters = Model(inputs=[input_text, input_cluster], outputs=output)\n",
    "model_clusters.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model_clusters.fit(\n",
    "    [X_train_pad, train_cluster_feat], \n",
    "    train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "predictions = (model_clusters.predict([X_test_pad, test_cluster_feat]) > 0.5).astype(\"int32\")\n",
    "print(\"Bi-LSTM + Clusters Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **5. Bi-LSTM with Hierarchical Cluster Features**\n",
    "* Use hierarchical clustering features instead, define another combined model, train and evaluate it, to report classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 179ms/step - accuracy: 0.9171 - loss: 0.2366 - val_accuracy: 0.9852 - val_loss: 0.0447\n",
      "Epoch 2/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 170ms/step - accuracy: 0.9912 - loss: 0.0268 - val_accuracy: 0.9874 - val_loss: 0.0331\n",
      "Epoch 3/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 181ms/step - accuracy: 0.9959 - loss: 0.0149 - val_accuracy: 0.9885 - val_loss: 0.0361\n",
      "Epoch 4/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 168ms/step - accuracy: 0.9949 - loss: 0.0136 - val_accuracy: 0.9855 - val_loss: 0.0489\n",
      "Epoch 5/5\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 169ms/step - accuracy: 0.9958 - loss: 0.0123 - val_accuracy: 0.9867 - val_loss: 0.0452\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step\n",
      "Bi-LSTM + Hierarchical Clusters Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      3309\n",
      "        spam       0.99      0.99      0.99      3434\n",
      "\n",
      "    accuracy                           0.99      6743\n",
      "   macro avg       0.99      0.99      0.99      6743\n",
      "weighted avg       0.99      0.99      0.99      6743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare cluster features (one-hot encoding)\n",
    "cluster_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# KMeans clusters as example \n",
    "train_cluster_feat = cluster_encoder.fit_transform(train_df[['hierarchical_cluster']])\n",
    "test_cluster_feat = cluster_encoder.transform(test_df[['hierarchical_cluster']])\n",
    "\n",
    "# Model definition (text + cluster)\n",
    "input_text = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
    "x = Bidirectional(LSTM(64))(embedding)\n",
    "\n",
    "# Cluster input\n",
    "input_cluster = Input(shape=(train_cluster_feat.shape[1],))\n",
    "\n",
    "# Concatenate clusters with Bi-LSTM output\n",
    "concatenated = Concatenate()([x, input_cluster])\n",
    "\n",
    "# Dense layers\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "model_clusters = Model(inputs=[input_text, input_cluster], outputs=output)\n",
    "model_clusters.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model_clusters.fit(\n",
    "    [X_train_pad, train_cluster_feat], \n",
    "    train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "predictions = (model_clusters.predict([X_test_pad, test_cluster_feat]) > 0.5).astype(\"int32\")\n",
    "print(\"Bi-LSTM + Hierarchical Clusters Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **6. Results**\n",
    "The results indicate great performance across all 3 models tested: the baseline Bi-LSTM model, Bi-LSTM with K-Means clustering features, and Bi-LSTM with Hierarchical clustering features. Each achieved accuracy, precision, recall, and F1-scores close to 100%, showing minimal performance differences. However, among them, the **Bi-LSTM with hierarchical clustering features** performed slightly better overall, reaching near-perfect precision, recall, and F1-scores (99%) for both ham and spam categories. The **Bi-LSTM with K-Means clustering features** was the next best performer, achieving nearly identical results to the hierarchical approach, while the **baseline Bi-LSTM** model had marginally lower but still exceptionally strong performance at approximately 98%. The negligible performance differences suggest that while clustering provided slightly improved contextual information to the models, the original text features alone were already highly discriminative. These minor differences emphasize the effectiveness of the baseline model and indicate that adding clustering features slightly enhances classification but isn't strictly necessary given the clear separability of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
